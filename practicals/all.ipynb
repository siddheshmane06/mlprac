{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4c6ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross valiation techniques\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "x = [10,20,30,40,50,60,70,80,90,100]\n",
    "# hold out cross validation method\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test = train_test_split(x, test_size=0.3)\n",
    "x_train\n",
    "x_test\n",
    "\n",
    "# leave one out cross validation\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "l = LeaveOneOut()\n",
    "for train, test in l.split(x):\n",
    "    print(train, test)\n",
    "    \n",
    "# leave P out cross validation\n",
    "from sklearn.model_selection import LeavePOut\n",
    "lpo = LeavePOut(1)\n",
    "print(lpo)\n",
    "x = np.array([[1,2],[3,4],[5,6],[7,8]])\n",
    "y = np.array([1,2,3,4])\n",
    "lpo = LeavePOut(2)\n",
    "for train_index, test_index in lpo.split(x):\n",
    "    print(\"Train:\", train_index, \"Test:\", test_index)\n",
    "    x_train, x_test = x[train_index], x[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "# K fold cross validation\n",
    "from sklearn.model_selection import KFold\n",
    "kf = KFold()\n",
    "x = ['a', 'b', 'c', 'd' ,'e', 'f']\n",
    "kf = KFold(n_splits = 3, shuffle = False, random_state = None)\n",
    "for train, test in kf.split(x):\n",
    "    print(\"Train data\", train, \"Test data\", test)\n",
    "\n",
    "# Stratified K fold cross validation\n",
    "from sklearn.model_selection import StratifiedKFold    \n",
    "x = np.array([[1,2],[3,4],[5,6],[7,8],[9,10],[11,12]])\n",
    "y = np.array([0,0,1,0,1,1])\n",
    "\n",
    "skf = StratifiedKFold(n_splits = 3, shuffle = False, random_state = None)\n",
    "\n",
    "for train_index, test_index in skf.split(x,y):\n",
    "    print(\"Train:\", train_index, \"Test:\", test_index)\n",
    "    x_train, x_test = x[train_index], x[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2128641f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#L2\n",
    "from sklearn.datasets import make_regression\n",
    "x, y = make_regression(n_samples=100, n_features=1, n_targets=1, noise=20, random_state=2)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(x,y)\n",
    "from sklearn.linear_model import LinearRegression\n",
    "lr = LinearRegression()\n",
    "lr.fit(x,y)\n",
    "lr.coef_\n",
    "lr.intercept_\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y,test_size =0.2)\n",
    "lr.fit(x_train,y_train)\n",
    "y_pred = lr.predict(x_test)\n",
    "plt.scatter(x_train, y_train, color='red') \n",
    "plt.plot(x_test,y_pred, color='blue')\n",
    "\n",
    "#Ridge Regression\n",
    "from sklearn.linear_model import Ridge\n",
    "r = Ridge(alpha = 10)\n",
    "r.fit(x,y)\n",
    "r.intercept_\n",
    "r.coef_\n",
    "\n",
    "rr = Ridge(alpha=100)\n",
    "rr.fit(x,y)\n",
    "print(rr.coef_)\n",
    "print(rr.intercept_)\n",
    "\n",
    "plt.plot(x,y,'b.')\n",
    "plt.plot(x,lr.predict(x),color='yellow',label='LR Line')\n",
    "plt.plot(x,r.predict(x),color='red',label='alpha=10')\n",
    "plt.plot(x,rr.predict(x),color='red',label='alpha=100')\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dca4ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Aim: Demonstrate how coefficient effected by increasing the values of the lambda (alpha)\n",
    "from sklearn.datasets import load_diabetes\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "data = load_diabetes()\n",
    "data\n",
    "df = pd.DataFrame(data.data, columns = data.feature_names)\n",
    "df[\"Target\"] = data.target\n",
    "df\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(data.data,data.target,test_size=0.2,random_state=2)\n",
    "x_train\n",
    "len(x_train)\n",
    "len(x_test)\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import r2_score\n",
    "coefs=[]\n",
    "r2_scores=[]\n",
    "for i in [0,10,100,1000]:\n",
    "    reg = Ridge(alpha=i)\n",
    "    reg.fit(x_train,y_train)\n",
    "    \n",
    "    coefs.append(reg.coef_.tolist())\n",
    "    y_pred = reg.predict(x_test)\n",
    "    r2_scores.append(r2_score(y_test,y_pred))\n",
    "coefs\n",
    "r2_scores\n",
    "plt.figure(figsize=(14,9))\n",
    "plt.subplot(221)\n",
    "plt.bar(data.feature_names,coefs[0])\n",
    "plt.title(\"Alpha=0,r2_score={}\".format(round(r2_scores[0],2)))\n",
    "\n",
    "plt.subplot(222)\n",
    "plt.bar(data.feature_names,coefs[1])\n",
    "plt.title(\"Alpha=10,r2_score={}\".format(round(r2_scores[1],2)))\n",
    "\n",
    "plt.subplot(223)\n",
    "plt.bar(data.feature_names,coefs[2])\n",
    "plt.title(\"Alpha=100,r2_score={}\".format(round(r2_scores[2],2)))\n",
    "\n",
    "plt.subplot(224)\n",
    "plt.bar(data.feature_names,coefs[3])\n",
    "plt.title(\"Alpha=1000,r2_score={}\".format(round(r2_scores[3],2)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b6460c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#In Ridge Regression prove that 'The more higher co-efficient are affected more'\n",
    "coefs=[]\n",
    "alphas = [0,0.0001,0.001,0.01,0.1,1,10,100,1000,10000]\n",
    "\n",
    "for i in alphas:\n",
    "    reg = Ridge(alpha=i)\n",
    "    reg.fit(x_train,y_train)\n",
    "    coefs.append(reg.coef_.tolist())\n",
    "np_arr = np.array(coefs)\n",
    "coef_df = pd.DataFrame(np_arr,columns=data.feature_names)\n",
    "coef_df['alpha'] = alphas\n",
    "coef_df.set_index('alpha')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168683f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#L1\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.datasets import make_regression\n",
    "x, y = make_regression(n_samples=100, n_features=5, n_informative=3, n_targets=1, noise=20, random_state=4)\n",
    "l = Lasso(alpha=0.01)\n",
    "l.fit(x,y)\n",
    "alphas = [0.001,0.01,0.1,1,2,3,4,5,6,15,20,30,50,80]\n",
    "for i in alphas:                                                                          \n",
    "    l = Lasso(alpha = i)\n",
    "    l.fit(x,y)\n",
    "    print(l.coef_)\n",
    "\n",
    "from sklearn.datasets import load_diabetes\n",
    "db = load_diabetes()\n",
    "df = pd.DataFrame(db.data, columns=db.feature_names)\n",
    "df.head()\n",
    "data = pd.DataFrame(db.data)\n",
    "target = pd.DataFrame(db.target)\n",
    "alphas = [0.001,0.01,0.1,1,2,3,4,5,6,15,20,30,50,80]\n",
    "for i in alphas:                                                                        \n",
    "    l = Lasso(alpha = i)\n",
    "    l.fit(data,target)\n",
    "    print(l.coef_)\n",
    "# Increasing the value of lambda results in coefficient = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7009103",
   "metadata": {},
   "outputs": [],
   "source": [
    "#bagging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "data = load_breast_cancer()\n",
    "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "df['target'] = data.target\n",
    "df.shape\n",
    "sample_1 = df.sample(200, replace = True)\n",
    "sample_2 = df.sample(200, replace = True)\n",
    "sample_3 = df.sample(200, replace = True)\n",
    "sample_1.shape\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "base_model_1 = DecisionTreeClassifier(max_depth=2)\n",
    "base_model_2 = DecisionTreeClassifier(max_depth=2)\n",
    "base_model_3 = DecisionTreeClassifier(max_depth=2)\n",
    "base_model_1\n",
    "sample_1.iloc[:,:30]\n",
    "base_model_1.fit(sample_1.iloc[:,:30],sample_1.iloc[:,-1])\n",
    "base_model_2.fit(sample_2.iloc[:,:30],sample_2.iloc[:,-1])\n",
    "base_model_3.fit(sample_3.iloc[:,:30],sample_3.iloc[:,-1])\n",
    "for i in df.sample():\n",
    "    print(i)\n",
    "df.sample().values\n",
    "df.drop(df['target'].values)\n",
    "sample_ = df.iloc[:,:30].sample(1).values\n",
    "#base_model_1.predict(df.iloc[:,:30].sample(1).values)\n",
    "base_model_1.predict(sample_)\n",
    "base_model_2.predict(sample_)\n",
    "base_model_3.predict(sample_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca71b198",
   "metadata": {},
   "outputs": [],
   "source": [
    "#stacking\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"data/heart.csv\")\n",
    "x = df.drop(columns=[\"target\"])\n",
    "y = df[\"target\"]\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 8)\n",
    "print(x_train.shape)\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "estimators = [\n",
    "    (\"rf\", RandomForestClassifier(n_estimators=10, random_state=42)),\n",
    "    (\"knn\", KNeighborsClassifier(n_neighbors=10)),\n",
    "    (\"gbdt\", GradientBoostingClassifier())\n",
    "]\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "\n",
    "clf = StackingClassifier(\n",
    "    estimators=estimators, \n",
    "    final_estimator=LogisticRegression(),\n",
    "    cv=10\n",
    ")\n",
    "clf.fit(x_train, y_train)\n",
    "y_pred = clf.predict(x_test)\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb0c288",
   "metadata": {},
   "outputs": [],
   "source": [
    "#k-means clustering\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "df=pd.read_csv('data/student_clustering.csv')\n",
    "plt.scatter(df['cgpa'],df['ML'])\n",
    "#define number of predefined cluster\n",
    "wcss=[]\n",
    "for i in range(1,11):\n",
    "    km=KMeans(n_clusters=i)\n",
    "    km.fit_predict(df)\n",
    "    wcss.append(km.inertia_)\n",
    "print(wcss)\n",
    "plt.plot(range(1,11),wcss)\n",
    "x=df.iloc[:,:].values\n",
    "km=KMeans(n_clusters=4)\n",
    "y_means=km.fit_predict(x)\n",
    "y_means\n",
    "x[y_means==0,1]\n",
    "plt.scatter(x[y_means==0,0],x[y_means==0,1],c='r')\n",
    "plt.scatter(x[y_means==1,0],x[y_means==1,1],c='b')   \n",
    "plt.scatter(x[y_means==2,0],x[y_means==2,1],c='g')\n",
    "plt.scatter(x[y_means==3,0],x[y_means==3,1],c='orange')   \n",
    "\n",
    "data=pd.read_csv('data/Density.csv')\n",
    "data.head(5)\n",
    "plt.scatter(data['0'],data['1'])\n",
    "wcss1=[]\n",
    "for i in range(1,11):\n",
    "    km=KMeans(n_clusters=i)\n",
    "    km.fit_predict(data)\n",
    "    wcss1.append(km.inertia_)\n",
    "print(wcss1)\n",
    "plt.plot(range(1,11),wcss1)\n",
    "x1=data.iloc[:,:].values\n",
    "km1=KMeans(n_clusters=3)\n",
    "y_means1=km1.fit_predict(x1)\n",
    "print(y_means1)\n",
    "x1[y_means1==0,1]\n",
    "plt.scatter(x1[y_means1==0,0],x1[y_means1==0,1],c='r')\n",
    "plt.scatter(x1[y_means1==1,0],x1[y_means1==1,1],c='b')   \n",
    "plt.scatter(x1[y_means1==2,0],x1[y_means1==2,1],c='g') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951ed525",
   "metadata": {},
   "outputs": [],
   "source": [
    "#agglomerative\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.cluster.hierarchy as sch\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "df = pd.read_csv(\"data/Mall_Customers.csv\")\n",
    "df.rename(columns={\"Annual Income (k$)\":\"annual_income\",\"Spending Score (1-100)\":\"score\"},inplace=True)\n",
    "data = df.iloc[:,-2:]\n",
    "data\n",
    "linkage_data = linkage(data, method='ward', metric='euclidean')\n",
    "dendrogram(linkage_data)\n",
    "plt.show()\n",
    "hierarchical_cluster = AgglomerativeClustering(n_clusters=5, affinity='euclidean', linkage='ward')\n",
    "labels = hierarchical_cluster.fit_predict(data)\n",
    "\n",
    "plt.scatter(data[\"annual_income\"],data[\"score\"],c=labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658eb1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dbscan\n",
    "import pandas as pd\n",
    "from sklearn.cluster import DBSCAN\n",
    "import matplotlib.pyplot as plt\n",
    "df = pd.read_csv(\"data/DBSCAN.csv\")\n",
    "df.rename(columns={\"A\":\"a\",\"B\":\"b\"},inplace=True)\n",
    "df.drop(columns=\"sr.no\",inplace=True)\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.scatter(df[\"a\"],df[\"b\"])\n",
    "db = DBSCAN(eps=30, min_samples=5)\n",
    "db.fit(df[[\"a\",\"b\"]])\n",
    "df[\"label\"] = db.labels_\n",
    "df.head()\n",
    "df['label'].unique()\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.scatter(df['a'],df['b'],c=df['label'])\n",
    "db.core_sample_indices_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ea487d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mba using apriori\n",
    "!pip install mlxtend\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "import matplotlib.pyplot as plt\n",
    "data = [\n",
    "    [\"Milk\",\"Onion\",\"Nutmeg\",\"Kidney Beans\",\"Yogurt\"],\n",
    "    [\"Chips\",\"Onion\",\"Nutmeg\",\"Kidney Beans\",\"Yogurt\"],\n",
    "    [\"Milk\",\"Corn Flour\",\"Sweet Corn\",\"Kidney Beans\",\"Yogurt\"],\n",
    "    [\"Milk\",\"Onion\",\"Spring Onion\",\"Kidney Beans\",\"Ice Cream\",\"Eggs\"]\n",
    "]\n",
    "type(data)\n",
    "te = TransactionEncoder()\n",
    "te_arry = te.fit(data).transform(data)\n",
    "te_arry\n",
    "df = pd.DataFrame(te_arry, columns=te.columns_)\n",
    "df\n",
    "# min_support=0.6 = 60%\n",
    "freq_item = apriori(df,min_support=0.6, use_colnames=True)\n",
    "freq_item\n",
    "# Association rule mining\n",
    "arm = association_rules(freq_item, metric=\"confidence\", min_threshold=0.7)\n",
    "arm\n",
    "rule = arm[[\"antecedents\",\"consequents\",\"support\",\"confidence\"]]\n",
    "rule\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1d585e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc67174",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
